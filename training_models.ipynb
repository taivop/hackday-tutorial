{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email spam detection using Python and scikit-learn\n",
    "### Helmes Hackday machine learning tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll learn about machine learning through a typical machine learning task: classification. In particular, we want to create a spam filter: for each email we want to predict whether it should be shown in the user's inbox (ham) or thrown away (spam).\n",
    "\n",
    "We will use the Enron spam classification dataset that you can download [here](http://www.aueb.gr/users/ion/data/enron-spam/). The folder structure should look like this:\n",
    "* data\n",
    "  * enron1\n",
    "    * ham\n",
    "      * 0001.1999-12-10.farmer.ham.txt\n",
    "      * 0002.1999-12-13.farmer.ham.txt\n",
    "      * ...\n",
    "    * spam\n",
    "      * 0006.2003-12-18.GP.spam.txt\n",
    "      * 0008.2003-12-18.GP.spam.txt\n",
    "      * ...\n",
    "  * enron2\n",
    "    * ham\n",
    "      * ...\n",
    "    * spam\n",
    "      * ...\n",
    "  * ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some imports. The most relevant to machine learning here are [**scikit-learn**](http://scikit-learn.org/stable/install.html) -- a standard library for training machine learning problems --, and **numpy** -- a very useful matrix computation library that scikit-learn depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import codecs\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import cross_validation\n",
    "from sklearn import grid_search\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's seed the pseudo-random generator so the results won't change on each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in all machine learning problems, we need to have some sort of numeric representation for our emails -- most machine learning models take numeric vectors as input. Let's write a very simple function for extracting some numbers (called features) from an email.\n",
    "\n",
    "These features are a reasonable guess -- spam often contains weird characters and a lot of capital letters -- but in principle we could plug in any numbers. If you think a feature could be useful, it's usually safer to use rather than omit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_features(text):\n",
    "    \"\"\"Extract some simple numeric features from one email.\"\"\"\n",
    "    length = len(text)\n",
    "    proportion_upper_case = sum(1 for c in text if c.isupper()) / length\n",
    "    proportion_lower_case = sum(1 for c in text if c.islower()) / length\n",
    "    proportion_alphanum = sum(1 for c in text if c.isalnum()) / length\n",
    "    return [length, proportion_upper_case, proportion_lower_case, proportion_alphanum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is scattered in a bunch of files, one for each email, however we would like to have a matrix of numbers where each row vector corresponds to the features we extracted from one email. Note that `feature_extraction_function` can be any function that returns a list of numbers, e.g. `stupid_features()`.\n",
    "\n",
    "We also need labels telling us whether each vector is spam or ham. Let's say we want to flag spam (the default is ham and spam is something extraordinary) so let's say spam corresponds to the label '1' and ham corresponds to '0'. This is a convention and we could do it the other way around with no difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(feature_extraction_function, number_of_emails=5):\n",
    "    \"\"\"Read all ham and spam emails from files and return a matrix of features and vector of labels.\"\"\"\n",
    "\n",
    "    # Data downloaded from\n",
    "    # http://www.aueb.gr/users/ion/data/enron-spam/\n",
    "\n",
    "    ham_pattern  = \"data/enron*/ham/*.txt\"\n",
    "    spam_pattern = \"data/enron*/spam/*.txt\"\n",
    "\n",
    "    # Initialise\n",
    "    features = []\n",
    "    labels   = []\n",
    "\n",
    "    # Find all files containing ham and spam emails and concatenate them with the corresponding label\n",
    "    ham_files = glob.glob(ham_pattern)\n",
    "    ham_files = ham_files[0:min(number_of_emails, len(ham_files))]\n",
    "    spam_files = glob.glob(spam_pattern)\n",
    "    spam_files = spam_files[0:min(number_of_emails, len(spam_files))]\n",
    "    files = list(zip(ham_files, [0] * len(ham_files))) + list(zip(spam_files, [1] * len(spam_files)))\n",
    "\n",
    "    for filename, label in files:\n",
    "        with codecs.open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = \"\".join(f.readlines())\n",
    "            email_features = feature_extraction_function(text)\n",
    "            labels.append(label)\n",
    "\n",
    "            features.append(email_features)\n",
    "\n",
    "    # Convert to Numpy objects because that's what scikit-learn expects\n",
    "    features = np.asarray(features)\n",
    "    labels   = np.asarray(labels)\n",
    "\n",
    "    print(\"Extracted features using function '%s'\" % (feature_extraction_function.__name__))\n",
    "    print(\"%d emails processed: %d ham, %d spam\" % (len(features), len(ham_files), len(spam_files)))\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's get our dataset and check what the first five training examples look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features using function 'simple_features'\n",
      "2000 emails processed: 1000 ham, 1000 spam\n",
      "[[  3.90000000e+01   2.56410256e-02   7.94871795e-01   8.20512821e-01]\n",
      " [  4.42900000e+03   2.25784601e-04   2.88326936e-01   3.59674870e-01]\n",
      " [  7.70000000e+01   1.29870130e-02   7.66233766e-01   7.92207792e-01]\n",
      " [  1.21900000e+03   8.20344545e-04   4.95488105e-01   5.52091879e-01]\n",
      " [  1.18300000e+03   8.45308538e-04   5.27472527e-01   5.78191040e-01]]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "NUM_EMAILS = 1000  # How many emails each of ham & spam do we want to use?\n",
    "features, labels = get_dataset(simple_features, number_of_emails=NUM_EMAILS)\n",
    "print(features[0:5,:])\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model: logistic regression\n",
    "Now that we have our data, it's time to do some machine learning. Let's start with a very simple model: [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) which is like linear regression (fitting a straight line through your data) with a small additional step that turns real numbers into 0/1 classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.684\n"
     ]
    }
   ],
   "source": [
    "model1 = LogisticRegression()\n",
    "model1.fit(features, labels)\n",
    "predicted_labels = model1.predict(features)\n",
    "training_accuracy = sklearn.metrics.accuracy_score(labels, predicted_labels)\n",
    "print(\"Accuracy on training set: %.3f\" % training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! What did we do?\n",
    "* initialised a logistic regression model,\n",
    "* told it to train itself (fit itself to the data),\n",
    "* asked the model what it would predict on the same data it trained on,\n",
    "* calculated the accuracy, i.e. percentage of correct guesses out of all guesses.\n",
    "\n",
    "## Accuracy, precision and recall\n",
    "\n",
    "Accuracy might not always be a good measure of how good a model is. In cancer detection, perhaps 0.01% of anyone you test actually has cancer and the rest are healthy. If you maximise accuracy, you can just say 'no-one has cancer' and your accuracy is 99.99%. In all cases where classes are imbalanced (i.e. the split isn't 50-50 cancer vs healthy) we want to use something more sophisticated.\n",
    "\n",
    "When we compare our predictions with the ground truth, we get four cases:\n",
    "* True Positive (TP): we correctly classified an email as spam.\n",
    "* True Negative (TN): we correctly classified an email as not spam.\n",
    "* False Positive (FP): we said the email was spam but it was actually ham.\n",
    "* False Negative (FN): we said the email was ham but it was actually spam.\n",
    "\n",
    "<img src=\"http://3.bp.blogspot.com/_txFWHHNYMJQ/THyADzbutYI/AAAAAAAAAf8/TAXL7lySrko/s1600/Picture+8.png\" alt=\"Confusion matrix\" style=\"height: 200px;\"/>\n",
    "\n",
    "This is why we use [**precision** and **recall**](https://en.wikipedia.org/wiki/Precision_and_recall). In our case, precision is the percentage of emails that we correctly classified as spam -- TP / (TP + FP) -- and recall is the percentage of spam emails that we correctly flagged as spam out of all emails that actually were spam -- TP / (TP + FN).\n",
    "\n",
    "It can take a while before these terms become more intuitive so keep the definitions handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.659, recall: 0.761\n"
     ]
    }
   ],
   "source": [
    "training_precision = sklearn.metrics.precision_score(labels, predicted_labels)\n",
    "training_recall = sklearn.metrics.recall_score(labels, predicted_labels)\n",
    "print(\"Precision: %.3f, recall: %.3f\" % (training_precision, training_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-test: in spam classification, users probably don't mind the occasional spam email in their inbox but are quite upset if a legitimate (and important) ham email gets sent to their spambox. Knowing this, which one is more important in spam classification: precision or recall?\n",
    "\n",
    "## F1 score\n",
    "\n",
    "It is kind of annoying to have two numbers we want to maximise -- it'd be much easier to aggregate precision and recall into a single number that we could track when training our models. This is what [F1 score](https://en.wikipedia.org/wiki/F1_score) does -- it is high iff both precision and recall are high. (If we wanted to we could also use a weighted F1 score that assigns more importance to precision but we'll skip that for simplicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.707\n"
     ]
    }
   ],
   "source": [
    "training_f1 = sklearn.metrics.f1_score(labels, predicted_labels)\n",
    "print(\"F1 score: %.3f\" % training_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and cross-validation\n",
    "Great! We've now trained a model and seen that it performs reasonably well. However, we don't really know if our model will do well on unseen data. How can we convince ourselves and the Product Manager that our spam detector is ready for production?\n",
    "\n",
    "We say that a model [**overfits**](https://en.wikipedia.org/wiki/Overfitting) if it does well on the training data but is really bad at predicting previously unseen examples. If you're studying for an exam and just study the previous year's exam questions rather than understand the material, you will probably do badly on the exam -- you overfit.\n",
    "\n",
    "How can we tell if we've overfitted our spam detector? The standard way is to randomly divide our training data into two batches, for example 80% and 20%; train our model on the 80% and test it on the 20%. If the model does well on the unseen 20% we can be reasonably sure that it will do well in production.\n",
    "\n",
    "\"But what if we randomly picked the 20% so that the model was very lucky and did well on the 20% by chance?\"\n",
    "\n",
    "I'm glad you asked. This is why we use [**cross-validation**](https://en.wikipedia.org/wiki/Cross-validation): we repeat this 80-20 split and training and evaluation five times -- each time picking a different (non-overlapping) 20% to test our model on. (This specifically is called k-fold cross-validation and in our case, k=5).\n",
    "\n",
    "\n",
    "If the above seemed complicated, you're in luck -- it's just one line with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 score in 5-fold cross-validation: 0.701\n"
     ]
    }
   ],
   "source": [
    "NUM_CV_FOLDS = 5\n",
    "model_cv = LogisticRegression()\n",
    "scores = cross_validation.cross_val_score(model_cv, features, labels, cv=NUM_CV_FOLDS, scoring='f1')\n",
    "print(\"Mean F1 score in %d-fold cross-validation: %.3f\" % (NUM_CV_FOLDS, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely better than [random](http://stats.stackexchange.com/questions/43102/good-f1-score-for-anomaly-detection) -- the F1 score of randomly guessing would be 0.5 in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our model\n",
    "\n",
    "How could we improve? Some of the first things to try are:\n",
    "1. Add more features.\n",
    "2. Find a better model (i.e. use something other than logistic regression).\n",
    "3. Find better hyperparameters for your model.\n",
    "\n",
    "Let's try adding more features. In theory, we're only limited by our imagination. In practice I would try to find things about the problem that you have a good hunch about (that you would use if you were to manually classify spam) and convert them into numbers.\n",
    "\n",
    "Our new feature extractor will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_features_extended(text):\n",
    "    \"\"\"Extract some more simple numeric features from one email.\"\"\"\n",
    "    count_free = text.count(\"free\")\n",
    "    count_credit = text.count(\"money\")\n",
    "    count_penis = text.count(\"penis\")\n",
    "    count_pill = text.count(\"pill\")\n",
    "    return [count_free, count_credit, count_penis, count_pill] + simple_features(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're using the old features as well.\n",
    "\n",
    "Let's calculate the dataset again, now with extended features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features using function 'simple_features_extended'\n",
      "2000 emails processed: 1000 ham, 1000 spam\n"
     ]
    }
   ],
   "source": [
    "features_ext, labels = get_dataset(simple_features_extended, number_of_emails=NUM_EMAILS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and plug them into a new LogisticRegression object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 score in 5-fold cross-validation: 0.706\n"
     ]
    }
   ],
   "source": [
    "model2 = LogisticRegression()\n",
    "scores = cross_validation.cross_val_score(model2, features_ext, labels, cv=NUM_CV_FOLDS, scoring='f1')\n",
    "print(\"Mean F1 score in %d-fold cross-validation: %.3f\" % (NUM_CV_FOLDS, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're doing very slightly better but not by much. Perhaps a more complex model could help us?\n",
    "\n",
    "## SVMs\n",
    "\n",
    "**Support vector machines** or [SVMs](https://en.wikipedia.org/wiki/Support_vector_machine) were the state of the art in many tasks before deep neural networks came around. Starting a machine learning task with training an SVM is often a good idea -- they are powerful and well understood so there are a lot of ~~StackOverflow answers~~ useful resources out there.\n",
    "\n",
    "So let's try an SVM predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features using function 'simple_features_extended'\n",
      "2000 emails processed: 1000 ham, 1000 spam\n",
      "Mean F1 score in 5-fold cross-validation: 0.503\n"
     ]
    }
   ],
   "source": [
    "features_ext, labels = get_dataset(simple_features_extended, number_of_emails=NUM_EMAILS)\n",
    "model3 = SVC()\n",
    "scores = cross_validation.cross_val_score(model3, features_ext, labels, cv=NUM_CV_FOLDS, scoring='f1')\n",
    "print(\"Mean F1 score in %d-fold cross-validation: %.3f\" % (NUM_CV_FOLDS, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We're doing much worse now -- no better than random guessing. What happened?\n",
    "\n",
    "This comes with experience, but it's likely that the problem is in the differing range of the features. SVMs are sensitive to the ranges of input features, so if one feature ranges (say) from 0 to 1 and another from 0 to 1 000 000, then SVMs don't work very well ([why?](http://stackoverflow.com/questions/15436367/svm-scaling-input-values)).\n",
    "\n",
    "Let's see if this is the case in our data by calculating the standard deviation over the first feature (# occurrences of the word \"free\") and the sixth feature (proportion of upper-case characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61322895398\n",
      "0.00801480700757\n"
     ]
    }
   ],
   "source": [
    "print(np.std(features_ext[:,0]))\n",
    "print(np.std(features_ext[:,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Clearly the fifth feature has much lower variance (variance = standard_deviation squared) so we need to do something about it.\n",
    "\n",
    "A reasonable default here is to do zero-mean unit-variance scaling, i.e. add something and multiply by something so that each feature has an arithmetic mean of 0 and a variance of 1.0. This is very easy to do in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 score in 5-fold cross-validation: 0.789\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_ext)\n",
    "model4 = SVC()  # Let's start with default parameters\n",
    "scores = cross_validation.cross_val_score(model4, features_scaled, labels, cv=NUM_CV_FOLDS, scoring='f1')\n",
    "print(\"Mean F1 score in %d-fold cross-validation: %.3f\" % (NUM_CV_FOLDS, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is much better! We went from near-random to the best result we've had so far.\n",
    "\n",
    "## Hyper-parameters and grid search\n",
    "\n",
    "Now that we know SVMs do work, we want to tinker with the **hyper-parameters**: these are parameters to the learning algorithm that control the learning process in some way (as opposed to 'regular' parameters that the learning algorithm is trying to optimise).\n",
    "\n",
    "In the case of the [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class (and actually all SVMs), the most important hyper-parameter is `C` which very informally punishes the learning algorithm for trying too hard: if `C` is very large, the algorithm will try really hard to classify all samples correctly (at the expense of possibly not generalising to unseen data) and if `C` is small, the algorithm will allow more mistakes but in return it will generalise better.\n",
    "\n",
    "The default value for `C` in `SVC` is 1.0. How do we find out which value is best?\n",
    "\n",
    "Basically, just trying many values. The fancy term here is **grid search** (as in a grid of values we want to try). We'll try a bunch of values that seem feasible, e.g. `[0.001, 0.01, 0.1, 1, 10, 100]`, and see what gives the best result.\n",
    "\n",
    "Again, this is very easy in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "model5 = SVC()\n",
    "gs = grid_search.GridSearchCV(model5, parameter_grid, cv=NUM_CV_FOLDS, verbose=1, scoring='f1')\n",
    "gs.fit(features_scaled, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which parameter gave the best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search scores for different values of C:\n",
      "mean: 0.71215, std: 0.02398, params: {'C': 0.001}\n",
      "mean: 0.71549, std: 0.02322, params: {'C': 0.01}\n",
      "mean: 0.74925, std: 0.03157, params: {'C': 0.1}\n",
      "mean: 0.78852, std: 0.02319, params: {'C': 1}\n",
      "mean: 0.78993, std: 0.02827, params: {'C': 10}\n",
      "mean: 0.78157, std: 0.03513, params: {'C': 100}\n"
     ]
    }
   ],
   "source": [
    "print(\"Grid search scores for different values of C:\")\n",
    "for score in gs.grid_scores_:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So the best was `C=10`. The standard recommendation is actually to do grid search roughly in steps of 0.3: `[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, ...]`. Obviously when you identify a more interesting range of values -- in our case roughly 1 to 10 -- you can try out more values in that range.\n",
    "\n",
    "For each of the 6 values of `C` we did a 5-fold cross-validation, so in total we trained `6 * 5 = 30` models. Keep in mind that this can get computationally expensive very quickly.\n",
    "\n",
    "## Applying the model to new emails\n",
    "\n",
    "Let's now take the best model we got out of grid search and do some classification with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_estimator = gs.best_estimator_\n",
    "\n",
    "ham_example = \"\"\"\n",
    "Subject: christmas break\n",
    "fyi\n",
    "- - - - - - - - - - - - - - - - - - - - - - forwarded by shirley crenshaw / hou / ect on 12 / 14 / 99\n",
    "07 : 51 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\" van t . ngo \" on 12 / 04 / 99 11 : 17 : 01 am\n",
    "to : vince j kaminski / hou / ect @ ect\n",
    "cc : shirley crenshaw / hou / ect @ ect\n",
    "subject : christmas break\n",
    "dear vince ,\n",
    "as the holidays approach , i am excited by my coming break from classes\n",
    "but also about the opportunity to see everyone at enron again and to\n",
    "work with you and them soon . i am writing to let you know that i would\n",
    "be very happy to work at enron over my break and i would like to plan\n",
    "out a schedule .\n",
    "my semester officially ends dec . 20 th but i may be out of town the week\n",
    "before christmas . i will be available the following three weeks , from\n",
    "monday , dec . 27 to friday , jan . 14 . please let me know if during those\n",
    "three weeks , you would like me to work and for what dates you would need\n",
    "the most help so that we can arrange a schedule that would be most\n",
    "helpful to you and so that i can contact andrea at prostaff soon .\n",
    "please let me know if you have any concerns or questions about a\n",
    "possible work schedule for me .\n",
    "give my regards to everyone at the office and wishes for a very happy\n",
    "holiday season ! i look forward to seeing you soon .\n",
    "sincerely ,\n",
    "van ngo\n",
    "ph : 713 - 630 - 8038\n",
    "- attl . htm\n",
    "\"\"\"\n",
    "\n",
    "spam_example = \"\"\"Subject: [ ilug ] bank error in your favor\n",
    "substantial monthly income makers voucher\n",
    "income transfer systems / distribution center\n",
    "pending income amount : up to $ 21 , 000 . 00\n",
    "good news ! you have made the substancial income makers list . this means you get the entire system and get the opportunity to make up to $ 21 , 000 . 00 a month .\n",
    "to receive this system , follow this link !\n",
    "get ready , you will immediately receive all the information needed to make a substantial monthly income .\n",
    "what are you waiting for ! ! http : / / www . hotresponders . com / cgi - bin / varpro / vartrack . cgi ? t = wendy 7172 : 1\n",
    "you are receiving this email due to having requested info on internet businesses . if you are not longer looking for one , please click the remove link below .\n",
    "click on the link below to remove yourself\n",
    "aol users\n",
    "remove me\n",
    "- -\n",
    "irish linux users ' group : ilug @ linux . ie\n",
    "http : / / www . linux . ie / mailman / listinfo / ilug for ( un ) subscription information .\n",
    "list maintainer : listmaster @ linux . ie\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each new email, we need to:\n",
    "1. turn the text into features using our feature extraction function (`simple_features_extended()`),\n",
    "2. reshape it to 2D instead of 1D so scikit-learn doesn't complain (`np.reshape()`),\n",
    "3. scale the features with the scaler we have from before (`scaler.transform()`),\n",
    "4. make the prediction. (`best_estimator.predict()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do we predict our ham example to be spam? 0\n"
     ]
    }
   ],
   "source": [
    "ham_example_features = np.asarray(simple_features_extended(ham_example))\n",
    "# Reshape because scikit-learn complains if you use a 1D shaped vector instead of 2D shaped one\n",
    "ham_example_features = np.reshape(ham_example_features, (1, -1))\n",
    "decision1 = best_estimator.predict(scaler.transform(ham_example_features))\n",
    "print(\"Do we predict our ham example to be spam? %d\" % decision1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do we predict our spam example to be spam? 1\n"
     ]
    }
   ],
   "source": [
    "spam_example_features = np.asarray(simple_features_extended(spam_example))\n",
    "spam_example_features = np.reshape(spam_example_features, (1, -1))\n",
    "decision2 = best_estimator.predict(scaler.transform(spam_example_features))\n",
    "print(\"Do we predict our spam example to be spam? %d\" % decision2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model is not perfect, it will sometimes fail, however these two emails it was able to classify correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to go next\n",
    "\n",
    "There are many ways you can try to improve on this (in no particular order):\n",
    "* Extracting probabilities not just binary decisions: use `predict_proba()`.\n",
    "* Adding more features you think could work.\n",
    "* Using some semantic features: [word embedding models](https://en.wikipedia.org/wiki/Word_embedding) allow us to convert each word into a (say 100-dimensional) vector that somehow captures the semantics of the word. Using these vectors as features in an SVM can potentially give you a model that 'understands' the text. Relevant Python package: [gensim](https://radimrehurek.com/gensim/).\n",
    "* Doing grid search also on other SVM parameters, not just `C` (hint: try changing the `kernel`).\n",
    "* Looking at the examples your model fails at and trying to find features specific to this.\n",
    "* Getting more data (in our case you can just increase the number of datapoints used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scikit-learn [documentation](http://scikit-learn.org/stable/).\n",
    "\n",
    "* Mailing list: [Data science weekly](http://www.datascienceweekly.org/).\n",
    "\n",
    "* Different (surprisingly easy) ways of getting started with deep neural networks: Tambet Matiisen's [presentation](http://www.meetup.com/Machine-Learning-Estonia/messages/boards/thread/49836878) at Estonian ML [meetup](http://www.meetup.com/Machine-Learning-Estonia).\n",
    "\n",
    "* Stanford MOOC on [Machine Learning](https://www.coursera.org/learn/machine-learning) (Coursera).\n",
    "\n",
    "* Standard textbook on Machine Learning: [Bishop (2007)](https://www.amazon.co.uk/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
